{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jax import nn, tree_leaves, random, numpy as jnp\n",
    "from jax_sgmc import data, potential, adaption, scheduler, integrator, solver\n",
    "import tensorflow as tf\n",
    "import haiku as hk\n",
    "\n",
    "from jax import config\n",
    "# config.update('TF_CPP_MIN_LOG_LEVEL', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "cached_batches = 128\n",
    "num_classes = 100\n",
    "weight_decay = 5.e-4\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': ShapeDtypeStruct(shape=(32, 32, 32, 3), dtype=uint8)}\n",
      "{'X': ShapeDtypeStruct(shape=(32, 32, 32, 3), dtype=uint8), 'Y': ShapeDtypeStruct(shape=(32, 1), dtype=int64)}\n",
      "{'X': ShapeDtypeStruct(shape=(32, 32, 32, 3), dtype=uint8)}\n",
      "{'X': ShapeDtypeStruct(shape=(32, 32, 32, 3), dtype=uint8), 'Y': ShapeDtypeStruct(shape=(32, 1), dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = \\\n",
    "    tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# TODO: Would be nice to use tensorflow dataloader here?\n",
    "train_loader = data.NumpyDataLoader(batch_size, X=train_images, Y=train_labels)\n",
    "test_loader = data.NumpyDataLoader(batch_size, X=test_images, Y=test_labels)\n",
    "\n",
    "train_batch_fn = data.random_reference_data(train_loader, cached_batches)\n",
    "\n",
    "# get first batch to init NN\n",
    "# TODO: Maybe write convenience function for this common usecase?\n",
    "batch_init, batch_get = train_batch_fn\n",
    "init_batch_state = batch_init()\n",
    "_, first_batch = batch_get(init_batch_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def init_resnet():\n",
    "    @hk.transform_with_state\n",
    "    def resnet(batch, is_training=True):\n",
    "        images = batch[\"X\"].astype(jnp.float32) / 255.\n",
    "        resnet50 = hk.nets.ResNet50(num_classes)\n",
    "        logits = resnet50(images, is_training=is_training)\n",
    "        return logits\n",
    "    return resnet.init, resnet.apply\n",
    "\n",
    "init, apply_resnet = init_resnet()\n",
    "init_params, init_resnet_state = init(random.PRNGKey(0), first_batch)\n",
    "\n",
    "# test prediction\n",
    "logits, _ = apply_resnet(init_params, init_resnet_state, None, first_batch)\n",
    "\n",
    "print(jnp.sum(logits))  # I don't think this should give plain 0, otherwise gradients will be 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize potential\n",
    "\n",
    "Everything below is still implemented without the state!\n",
    "Can we somehow provide additional arguments to likelihood?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def likelihood(sample, observations):\n",
    "    labels = nn.one_hot(observations[\"Y\"], num_classes)\n",
    "    logits, resnet_state = apply_resnet(sample[\"w\"], resnet_state, observations[\"X\"])\n",
    "    softmax_xent = -jnp.sum(labels * nn.log_softmax(logits))\n",
    "    softmax_xent /= labels.shape[0]\n",
    "    return softmax_xent\n",
    "\n",
    "\n",
    "def prior(sample):\n",
    "    # Implement weight decay, corresponds to Gaussian prior over weights\n",
    "    weights = sample[\"w\"]\n",
    "    l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in tree_leaves(weights))\n",
    "    return weight_decay * l2_loss\n",
    "\n",
    "potential_fn = potential.minibatch_potential(prior=prior,\n",
    "                                             likelihood=likelihood,\n",
    "                                             strategy=\"vmap\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Integrator\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "iterations = 50000\n",
    "\n",
    "# Adaption strategy\n",
    "rms_prop = adaption.rms_prop()\n",
    "\n",
    "# Integrators\n",
    "rms_integrator = integrator.langevin_diffusion(potential_fn,\n",
    "                                               train_batch_fn,\n",
    "                                               rms_prop)\n",
    "\n",
    "# Initial value for starting\n",
    "sample = {\"w\": init_params}\n",
    "\n",
    "# Schedulers\n",
    "rms_step_size = scheduler.polynomial_step_size_first_last(first=0.05,\n",
    "                                                          last=0.001)\n",
    "burn_in = scheduler.initial_burn_in(10000)\n",
    "rms_random_thinning = scheduler.random_thinning(rms_step_size, burn_in, 4000)\n",
    "\n",
    "rms_scheduler = scheduler.init_scheduler(step_size=rms_step_size,\n",
    "                                         burn_in=burn_in,\n",
    "                                         thinning=rms_random_thinning)\n",
    "\n",
    "rms_sgld = solver.sgmc(rms_integrator)\n",
    "rms_run = solver.mcmc(rms_sgld, rms_scheduler)\n",
    "rms = rms_run(rms_integrator[0](sample), iterations=iterations)[\"samples\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}